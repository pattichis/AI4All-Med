{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pattichis/AI4All-Med/blob/main/Session_4_2_Medichat_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medichat Example\n",
        "\n",
        "We found an open-source medical LLM through HuggingFace: https://huggingface.co/collections/sethuiyer/medical-llms.\n",
        "\n",
        "Below is the associated example code on their model website: https://huggingface.co/sethuiyer/Medichat-Llama3-8B.\n",
        "\n",
        "_For this to run faster, go to Runtime > Change Runtime Type, and select a GPU option (with high-RAM if available)._"
      ],
      "metadata": {
        "id": "d0AYw8o1DFmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experiments / Questions**\n",
        "We learned that prompts can have the following list of instructions: persona, instruction, context, format, audience, tone, and data (that the model should perform the instruction on).\n",
        "\n",
        "1. What subset of the above list make sense to belong in the sys_message?\n",
        "  - Hint: This message gets prepended to every user input.\n",
        "2. Modify the question. How well does it do on your medical question?\n",
        "3. Modify the sys_message. How does this change the perdformance on your previously run question?"
      ],
      "metadata": {
        "id": "qPXDGNuyFoMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "VudEzvUOp0MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "y_ukWEK8D040"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "class MedicalAssistant:\n",
        "    def __init__(self, model_name=\"sethuiyer/Medichat-Llama3-8B\", device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            # device='auto',\n",
        "            quantization_config=quantization_config\n",
        "            )#.to(self.device)\n",
        "        self.sys_message = '''\n",
        "        You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and\n",
        "        provide an informative answer. If you don't know the answer to a specific medical inquiry, advise seeking professional help.\n",
        "        '''\n",
        "\n",
        "    def format_prompt(self, question):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.sys_message},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, question, max_new_tokens=512):\n",
        "        prompt = self.format_prompt(question)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
        "        answer = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "        return answer"
      ],
      "metadata": {
        "id": "NIGkx5A9D1sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NOTE: Only run the cell below once or else your runtime will crash!**"
      ],
      "metadata": {
        "id": "KN0Mp_uUFJlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = MedicalAssistant()"
      ],
      "metadata": {
        "id": "K4QNfzavEMpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IogK8qLCzu1"
      },
      "outputs": [],
      "source": [
        "question = '''\n",
        "Symptoms:\n",
        "Dizziness, headache, and nausea.\n",
        "\n",
        "What is the differential diagnosis?\n",
        "'''\n",
        "response = assistant.generate_response(question)\n",
        "print(response)"
      ]
    }
  ]
}